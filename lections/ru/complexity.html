<h2>Вступление</h2>

<p>Наверное, каждый из нас на начальных этапах сталкивался с ситуацией вида
"я решил эту задачу, но она не проходит по времени". Это может озадачивать:
вы написали то, что от вас требовалось по условию, проверили свой код, но
решение почему-то не укладывается в ограничение по времени (о котором вы раньше
даже не задумывались). Причина этого явления проста: вы перешли с уровня
задач на "реализацию" на уровень задач на "идею". Это значит, что просто
написать то, что сказано в условии теперь недостаточно. Вам нужно начать
задумываться об <em>эффективности</em> вашего решения.</p>

<p>В чём же выражается эффективность? Чаще всего самый важный параметр &ndash;
время работы программы. Стоит сразу заметить, что в большинстве случаев
время работы оптимального и наивного решения отличается далеко не в 2 раза.
Наивное решение для многих задач может работать минуты, часы, года, а часто
и вовсе астрономические величины. Причины такого поведения мы рассмотрим ниже.
Другим важным параметром является количество использованной памяти, но оно
редко становится ключевым для эффективности программы. Если ваше решение не
укладывается в ограничение по памяти, оно почти наверняка не уложится и
в ограничение по времени.</p>

<h2>Пример задачи</h2>

<p>В качестве примера рассмотрим следующую задачу:</p>

<blockquote>
Дан массив из $N$ натуральных чисел ($N \le 10^6$). Числа не превышают $10^6$.
Сколько пар равных чисел в нём содержится?
</blockquote>

<p>Казалось бы, что может быть проще? Недолго думая вы пишите код такого вида:</p>

<!--@Listing:array-equal-pairs/naive/ru/**/*-->

<p>И получаете вердикт TL (Time Limit exceeded). Давайте разберёмся почему:
посчитаем, сколько операций выполнит наш алгоритм в худшем случае (при $N = 10^6$).
Рассмотрим первую итерацию внешнего цикла ($i = 0$). Во внутреннем цикле переменная
$j$ будет принимать по очереди все значения от $1$ до $N - 1$, то есть, тело
внутреннего цикла выполнится $N - 1$ раз. На второй итерации внешнего цикла число
итераций внутреннего будет равно $N - 2$. На третьей $N - 3$. И так далее, пока
это число не достигнет нуля. Пренебрегая операциями ввода и вывода, мы можем
выразить общее число итераций следующей формулой:</p>

$$(N - 1) + (N - 2) + \ldots + 1$$

<p>Преобразуем её в сумму натурального ряда и посчитаем значение:</p>

$$(N - 1) + (N - 2) + \ldots + 1 = \\
= 1 + 2 + \ldots + (N - 1) = \\
= \sum\limits_{i = 1}^{N - 1} i = \\
= {N * (N - 1) \over 2}$$

<p>В худшем случае это будет равно</p>

$${10^6 * (10^6 - 1) \over 2} = 499999500000 \approx 5 * 10^{11},$$

<p>Или примерно пятьсот миллиардов. Много, не так ли? В 2015-м году обычный
компьютер может выполнять примерно $10^7$-$10^8$ операций в секунду.
Стоит заметить, что не все операции равнозначны. С $10^8$ операций сложения
любой компьютер справится без проблем, а, например, взятия квадратного корня,
уже не каждый. Принимая скорость компьютера за $10^8$ операций/секунду, наша
программа выполнится за каких-то $5000$ секунд, примерно $1.5$ часа. Стоит ли
говорить, что это неприемлемо?</p>

<p>Как же решать эту задачу? Нужно придумать алгоритм, который будет работать
гораздо быстрее. В таких ситуациях всегда нужно внимательно проанализировать
ограничения на входные данные. Кроме ограничения на размер массива, у нас
также ограничены сами числа, они не превышают $10^6$. Можно ли это как-то
использовать? Да. Посчитаем вспомогательный массив $c$, где $c[i]$ &ndash; количество чисел $i$
в изначальном массиве. Если в массиве пять троек, то $c[3] = 5$. Как в таком случае
получить количество пар троек? Владея элементарной комбинаторикой, или
черновиком с ручкой, можно заметить, что количество всех пар связано с
количеством элементов формулой</p>

$$P = {x * (x - 1) \over 2}.$$

<p>Значит, мы можем найти количество пар троек:</p>

$$P = {5 * 4 \over 2} = 10$$

<p>Посчитаем этот параметр для всех чисел и сложим получившиеся результаты.
Это и будет ответом на задачу.</p>

<!--@Listing:array-equal-pairs/good/ru/**/*-->

<p>Как видите, мы уложились в два цикла, в худшем случае каждый из них
выполнит по миллиону итераций, и общее число операций будет порядка $3 * 10^6$.
Значит, на том же компьютере наша программа будет выполняться за $30$ мс, что
уже гораздо лучше.</p>

<h2>Асимптотическая сложность</h2>

<p>На практике точно оценить время работы алгоритма почти что невозможно.
Даже если вы точно подсчитаете все элементарные операции (инструкции машинного
кода) вашей программы (что уже неоправданно сложно), каждая инструкция
выполняется процессором за разное количество тактов. Процессор часто выполняет
несколько операций сложения за такт, а вот для операции деления требуется около
десяти тактов. Время выполнения одного такта также может варьироваться
(процессор не всегда работает на заявленной тактовой частоте). А если вспомнить
про многопоточность и многоядерность, то перспективы окончательно омрачняются.
Поэтому точным временем работы в информатике не оперируют.</p>

<p><strong>Вместо этого используется зависимость времени работы от входных данных.
</strong> В предыдущем разделе мы считали так называемое "примерное количество
операций". Принимая, что время работы прямо пропорционально количеству операций,
запишем время работы наивного алгоритма в таком виде:</p>

$$t(N) = {N * (N - 1) \over 2} + N + c$$

<p>Слагаемое $N$ учитывает цикл для ввода массива, а $c$ &ndash; некоторая
константа, описывающая количество операций вне всех циклов. Преобразуем эту
формулу в стандартный вид:</p>

$$t(N) = {N^2 \over 2} + {N \over 2} + c$$

<p>Теперь необходимо ввести понятие <strong>"O большое"</strong>. "О большое" &ndash;
математический способ <em>приблизительной</em> оценки функции. Запись выглядит
следующим образом:</p>

$$f(x) = O(g(x))$$

<p>И точно означает следующее утверждение:</p>

<blockquote>
Существуют такие числа $M$ и $C$, что выполняется утверждение:
$$f(x) &lt; C * g(x),\ для\ всех\ x &gt; M$$
</blockquote>

<p>Или, перефразируя, для достаточно больших $x$, $f(x)$ <em>примерно равно</em>
$g(x)$, или, $f(x)$ и $g(x)$ отличаются <em>на константу</em>.</p>

<p>Применив такую запись к нашему времени работы мы получим:</p>

$$t(N) = O(N^2)$$

<p>Мы откинули члены $N$ и $c$, потому что при достаточно больших $N$ они
бесконечно малы по сравнению с $N^2$. Деление на $2$ откидывается, так как
это, по сути, умножение на константу ${1 \over 2}$, а константа нас не
интересует при примерной оценке.</p>

<p>Строго говоря, у зависимости времени работы от входных данных есть
собственное название. Этот параметр называется <strong>асимптотической
сложностью</strong>, или просто сложностью. Можно сказать, что сложность
рассматриваемого алгоритма $O(N^2)$, или что у алгоритма квадратичная сложность.
На практике понятия "время работы", "сложность" и "асимптотика" используются
как взаимозаменяемые.</p>

<p>Используя новые термины, можно сказать, что у наивного решения этой задачи
сложность $O(N^2)$, а у оптимального $O(N)$.</p>

<p>Чаще всего встречаются следующие значения асимптотики:</p>

<ul>
    <li>$O(1)$</li>
    <li>$O(\log N)$</li>
    <li>$O(\log^2 N)$</li>
    <li>$O(\sqrt[\leftroot{2} 3] N)$</li>
    <li>$O(\sqrt N)$</li>
    <li>$O(N)$</li>
    <li>$O(N \log N)$</li>
    <li>$O(N \log^2 N)$</li>
    <li>$O(N \sqrt N)$</li>
    <li>$O(N^2)$</li>
    <li>$O(N^2 \log N)$</li>
    <li>$O(N^3)$</li>
    <li>$O(2^N)$</li>
    <li>$O(N!)$</li>
</ul>

<p>Иногда нужно подчеркнуть, что несмотря на то, что два алгоритма имеют
одинаковую сложность, один из них работает заметно быстрее другого. В
таких случаях говорят, что у такого алгоритма <em>ниже константа</em>.

<h2>Оценка используемой памяти</h2>

<p>Асимптотическая сложность может использоваться также для оценки зависимости
используемой памяти от входных данных, но на практике это встречается гораздо
реже. Практически вся используемая память чаще всего уходит на один или
несколько крупных массивов, длина которых обычно константа (при решении на C++). Зная размер
элементарных типов можно тривиально вычислять размер массивов. Например, в C++
массив из миллиона <code>long long</code> занимает в памяти 8 МБ, так как
размер типа <code>long long</code> равен 8 байт.</p>
